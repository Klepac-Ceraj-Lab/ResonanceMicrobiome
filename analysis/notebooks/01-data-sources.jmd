---
title: "Notebook 1: Data Sources"
author: "Kevin Bonham, PhD"
options:
    line_width : 120
    wrap : false
---

## Description

Raw data for this paper can be [found here](https://zenodo.org/record/3633793).

In order to perform the analyses outlined in the notebooks in this project,
you'll need to download and unpack these files
and put it into directories that are accessible.

By default, data generated from the `bioBakery` toolset
(those found in the `batchXXX_analysis_noknead.tar.gz` archives)
are expected to be placed in `data/engaging/` folders.
Paths for other data may be changed in the `data.toml` file
found in the `data/` directory.

Some of these paths also set where files will be created.
In particular, you'll likely need to change the paths
for the `sqlite` databases.

## Metadata

Patient metadata is kept in a FileMaker Pro database,
and exported into a file that requires some effort to parse effectively.

If you have downloaded data from Zenodo,
you may skip ahead to [Creating SQLite databases](#creating-sqlite-databases).
These next sections document how subject metadata
is extracted from the internal FilemakerPro database.

### Export from FilemakerPro

The first step is to
export the relevant sections of the database as `.xlsx` (MS Excel) files.
First, make sure all records are being displayed
by clicking the `Show All` button (if it is not greyed out).

![Show all button](../src/img/01.showall.png)

Next, export desired sections as an excel file.
The following demonstrates the steps
to get the metadata associated with timepoints,

1. Select `File > Export Records`
1. Select the location to save the file (eg `~/Desktop/timepoints`),
    and click `Save`
1. When the `Excel Options` dialogue box pops up, click `Continue`
1. In the `Specify Field Order for Export` dialogue box:
    1. If you've previously exported anything, clear those files
        by clicking `Clear All`
    1. Select a table to export.
        In this example, we'll select `Timpointinfo`
    1. Then click `Move All`
    1. Repeat for additional tables/fields, then `Export`

![Export](../src/img/01.export.png)

![Save](../src/img/01.save.png)

![Clear previous](../src/img/01.clearall.png)

![Table select](../src/img/01.tableselect.png)

![Move all and export](../src/img/01.moveexport.png)

### Metadata Scrub

Once you have this file,
You'll need to run the `metascrub.jl` script contained in the `bin/` folder
of the [`ECHOAnalysis`](https://doi.org/10.5281/zenodo.3648679) repository.

This script does a few things.
First, it separates each parent table
(that is, the stuff before the `::`, eg `TimepointInfo`).

Second, it removes empty columns and rows
(that is, columns and rows that have only missing values).

Third, some custom processing is done for certain tables -
for example,
`collectionNum` in `FecalSampleCollection`
is renamed to `timepoint` to be consistent with other tables.
See the `customprocess()`function inside the script
to see the table-specific steps.

Finally, the data is converted to long form.

```
| subject  | timepoint | metadatum | value    | parent_table |
|----------|-----------|-----------|----------|--------------|
| Int      | Int       | String    | Any      | String       |
```

NOTE: Metadata from tables that don't have timepoints
(ie things that are the same at all timepoints)
are given a timepoint of `0`.

For more information, run the script with the `--help`:

```
$ julia --project=@. $REPO_PATH/bin/metascrub.jl --help
usage: metascrub.jl [-d] [-v] [-q] [-l LOG] [--delim DELIM]
                    [--sheet SHEET] [--dry-run] [-o OUTPUT]
                    [-s SAMPLES] [-h] input

positional arguments:
  input                 Table to be scrubbed. By default, this will be
                        overwritten if a CSV file

optional arguments:
  -d, --debug           Show @debug level logging messages
  -v, --verbose         Show @info level logging messages
  -q, --quiet           Only show @error logging messages
  -l, --log LOG         Write logs to this file. Default - no file
  --delim DELIM         for tabular text files, the delimeter used
                        (generally ',' or '     ') (default: ",")
  --sheet SHEET         for xlsx files, the name of the sheet that
                        data is stored on (default: "Sheet1")
  --dry-run             Show logging, but take no action. Most useful
                        with --verbose
  -o, --output OUTPUT   Output for scrubbed file (defaults to
                        overwriting input)
  -s, --samples SAMPLES
                        Path to sample metadata to be included
                        (optional)
  -h, --help            show this help message and exit
```

For example, to run this script on our example above:

```
$ julia --project=@. $REPO_PATH/bin/metascrub.jl data/metadata/filemakerall.xlsx \
    -vl data/metadata/filemakerdb.log \
    -o data/metadata/filemakerdb.csv
```

Additional transformations of the metadata to get it into a usable form
can be found in the next notebook.

## Metagenome data

Compressed quality-scored sequencing files (`.fastq.gz`)
from the sequencing facility were concatenated
and run through the bioBakery metagenome workflow.

This repository contains samples from sequencing batches 001-012.

### Running the snakemake workflow

[Github repository link.](https://github.com/Klepac-Ceraj-Lab/snakemake_workflows)

A custom snakemake workflow was used on all samples
with the following software versions:

- kneaddata v0.7.1
- metaphlan2 v2.7.7
- humann2 v0.11.1

The following command was run on the `engaging` compute cluster (`eofe7.mit.edu`)
from MIT (a Centos7 environment).

```
$ snakemake -s /home/vklepacc/software/repos/snakemake_workflows/biobakery_all.snakefile \
    --configfile config.yaml --cluster-config cluster.yaml \
    --cluster "sbatch -n {cluster.processors} -N {cluster.nodes} -t {cluster.time} --mem {cluster.memory} -o output/logs/{rule}-%j.out -e output/logs/{rule}-%j.err -p newnodes --exclude=node119" \
    --latency-wait 15
```

## Adding Metadata to SQLite databases

The following code (and code from all subsequent notebooks)
assumes that your current working directory is that of this repository.
I executed the code directly from the markdown file
using [Juno](https://junolab.org/) IDE extension for the Atom text editor.

You may also copy and paste the following code into a `julia` REPL
or convert these files into jupyter notebooks
using the [Weave.jl](http://weavejl.mpastell.com/dev/notebooks/#Output-to-Jupyter-notebooks-1) package.

If you run into path errors,
check that `data/data.toml` has the correct paths
to the required files and directories.

```julia; echo=false; results="hidden"
using Pkg
Pkg.activate("analysis")
using Revise
```

```julia
using SQLite
using CSV
using Pkg.TOML: parsefile

config = parsefile("data/data.toml")

metadb = SQLite.DB(config["sqlite"]["metadata"]["path"])
SQLite.drop!(metadb, "filemakermetadata", ifexists=true)
SQLite.dropindex!(metadb, "filemakermetadata_subject_idx", ifexists=true)
SQLite.dropindex!(metadb, "filemakermetadata_metadatum_idx", ifexists=true)

CSV.File(config["tables"]["metadata"]["path"]) |> SQLite.load!(metadb, "filemakermetadata")

SQLite.createindex!(metadb, "filemakermetadata", "filemakermetadata_subject_idx", "subject", unique=false)
SQLite.createindex!(metadb, "filemakermetadata", "filemakermetadata_metadatum_idx", "metadatum", unique=false)
```


## Adding Taxonomic and Functional profiles to sqlite database

The following steps can take quite a while,
especially adding the `genefamilies` tables.


```julia
using SQLite
using CSV
using Pkg.TOML: parsefile
using ECHOAnalysis

config = parsefile("data/data.toml")

taxdb = SQLite.DB(config["sqlite"]["taxa"]["path"])
add_taxonomic_profiles(taxdb, config["files"]["biobakery"]["path"], replace=true)

allsamples = readdir(config["files"]["rawfastq"]["path"])
filter!(isstoolsample, allsamples)
allsamples = unique(stoolsample.(allsamples))
samples = uniquetimepoints(allsamples, takefirst=false)

funcdb = SQLite.DB(config["sqlite"]["uniref90"]["path"])
add_functional_profiles(funcdb, config["files"]["biobakery"]["path"], kind="genefamilies_relab", replace=true, samples=samples)
kodb = SQLite.DB(config["sqlite"]["ko"]["path"])
add_functional_profiles(kodb, config["files"]["biobakery"]["path"], kind="ko_names_relab", replace=true, samples=samples)
pfamdb = SQLite.DB(config["sqlite"]["pfam"]["path"])
add_functional_profiles(pfamdb, config["files"]["biobakery"]["path"], kind="pfam_names_relab", replace=true, samples=samples)
ecdb = SQLite.DB(config["sqlite"]["ec"]["path"])
add_functional_profiles(ecdb, config["files"]["biobakery"]["path"], kind="ec_names_relab", replace=true, samples=samples)
```
